{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "99af4aef",
   "metadata": {},
   "source": [
    "# N-Gram Model from Report Descriptions\n",
    "\n",
    "This notebook create a **n-gram model** from the `Description` column using `nltk`, in order to expand the `keywords` used in search."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "04854cda",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/cbadenes/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/cbadenes/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Install required NLTK resources (if running for the first time)\n",
    "# !pip install nltk\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "# Load libraries\n",
    "import pandas as pd\n",
    "import re\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.util import ngrams\n",
    "from nltk import FreqDist\n",
    "from collections import defaultdict\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning, module=\"openpyxl\")\n",
    "from pathlib import Path\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e1d9045",
   "metadata": {},
   "source": [
    "## Load and prepare report descriptions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "aefb0e9c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Methodolody and definition of the algorithim of Feeder Market'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load Views sheet\n",
    "views_path = Path(\"../raw/Reporting_Inventory.xlsx\")\n",
    "views_df = pd.read_excel(views_path, sheet_name=\"Views\")\n",
    "views_df.fillna(\"\", inplace=True)\n",
    "views_df = views_df[views_df[\"Description\"].str.strip() != \"\"]\n",
    "descriptions = views_df[\"Description\"].dropna().astype(str).tolist()\n",
    "descriptions[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc22334b",
   "metadata": {},
   "source": [
    "### Preprocess text: lowercase, clean, remove stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "07b08850",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define English stopwords (can switch to 'spanish' if needed)\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "def preprocess(text):\n",
    "    \"\"\"\n",
    "    Normalize, tokenize, and clean a single string of text.\n",
    "    \"\"\"\n",
    "    # Lowercase and remove punctuation\n",
    "    text = text.lower()\n",
    "    text = re.sub(r\"[^a-záéíóúñü\\s]\", \"\", text)\n",
    "    \n",
    "    # Tokenize and remove stopwords\n",
    "    tokens = word_tokenize(text)\n",
    "    filtered_tokens = [t for t in tokens if t not in stop_words]\n",
    "    \n",
    "    return filtered_tokens\n",
    "\n",
    "# Apply preprocessing to each report description\n",
    "token_lists = [preprocess(desc) for desc in descriptions]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "516864bd",
   "metadata": {},
   "source": [
    "### Generate and analyze bigrams and trigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3a91fda4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 10 bigrams:\n",
      "[(('index', 'page'), 51), (('buttons', 'views'), 50), (('page', 'interactive'), 48), (('interactive', 'buttons'), 48), (('feeder', 'market'), 44), (('first', 'block'), 39), (('detail', 'regarding'), 37), (('block', 'filters'), 35), (('view', 'analyze'), 34), (('dynamic', 'table'), 31)]\n",
      "\n",
      "Top 10 trigrams:\n",
      "[(('index', 'page', 'interactive'), 48), (('page', 'interactive', 'buttons'), 48), (('interactive', 'buttons', 'views'), 48), (('first', 'block', 'filters'), 35), (('contains', 'first', 'block'), 21), (('month', 'week', 'evolution'), 19), (('quest', 'detail', 'regarding'), 17), (('view', 'shows', 'first'), 15), (('shows', 'first', 'block'), 15), (('block', 'filters', 'second'), 15)]\n"
     ]
    }
   ],
   "source": [
    "# Flatten all tokens into a single list\n",
    "all_tokens = [token for tokens in token_lists for token in tokens]\n",
    "\n",
    "# Build bigrams and trigrams\n",
    "bigrams = list(ngrams(all_tokens, 2))\n",
    "trigrams = list(ngrams(all_tokens, 3))\n",
    "\n",
    "# Compute frequency distributions\n",
    "bigram_freq = FreqDist(bigrams)\n",
    "trigram_freq = FreqDist(trigrams)\n",
    "\n",
    "# Preview top 10 n-grams\n",
    "print(\"Top 10 bigrams:\")\n",
    "print(bigram_freq.most_common(10))\n",
    "\n",
    "print(\"\\nTop 10 trigrams:\")\n",
    "print(trigram_freq.most_common(10))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7a2f0347",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "# Build bigram prediction index: word -> list of possible next words sorted by frequency\n",
    "bigram_index = defaultdict(list)\n",
    "for (w1, w2), freq in bigram_freq.items():\n",
    "    bigram_index[w1].append((w2, freq))\n",
    "\n",
    "# Sort each list of next-word suggestions by frequency\n",
    "for w1 in bigram_index:\n",
    "    bigram_index[w1] = sorted(bigram_index[w1], key=lambda x: -x[1])\n",
    "\n",
    "# Convertir a dict serializable\n",
    "serializable_index = {k: v for k, v in bigram_index.items()}\n",
    "\n",
    "# Save dictionary as JSON\n",
    "with open(\"../models/bigram_index.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(serializable_index, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "81f03ba9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Suggestions for 'feeder': ['market']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def suggest_next_word(word, top_n=1):\n",
    "    \"\"\"\n",
    "    Given a word, suggest the most likely next tokens based on bigram frequencies.\n",
    "    \"\"\"\n",
    "    word = word.lower()\n",
    "    if word in bigram_index:\n",
    "        return [next_word for next_word, _ in bigram_index[word][:top_n]]\n",
    "    else:\n",
    "        return []\n",
    "\n",
    "# Example usage:\n",
    "ref_word = \"feeder\"\n",
    "print(\"Suggestions for '\"+ref_word+\"':\", suggest_next_word(ref_word))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aee858bb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "07ac5f62",
   "metadata": {},
   "source": [
    "# Train Custom Word Embeddings from PDF Corpus\n",
    "\n",
    "In this notebook, we train a custom word embedding model using text extracted from a folder of PDF documents. The resulting embeddings can later be loaded and used with Flair for downstream tasks like similarity search, clustering, etc.\n",
    "\n",
    "We will use `PyMuPDF` to extract text from PDFs and `Gensim` to train a Word2Vec model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "15ce2361",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/cbadenes/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "from pathlib import Path\n",
    "import fitz  # PyMuPDF\n",
    "from gensim.models import Word2Vec\n",
    "from nltk.tokenize import word_tokenize\n",
    "import nltk\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b168cbaa",
   "metadata": {},
   "source": [
    "## Extract and clean text from PDF files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "e5137294",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reading ../pdf/USALI - 11th Edition.pdf\n",
      "Extracted text from 1 PDFs.\n"
     ]
    }
   ],
   "source": [
    "# Path to folder containing PDF files\n",
    "pdf_folder = Path(\"../pdf\")\n",
    "documents = []\n",
    "\n",
    "for pdf_file in pdf_folder.glob(\"*.pdf\"):\n",
    "    print(\"reading\",pdf_file)\n",
    "    text = \"\"\n",
    "    with fitz.open(pdf_file) as doc:\n",
    "        for page in doc:\n",
    "            text += page.get_text()\n",
    "    # Basic cleanup\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "    documents.append(text)\n",
    "\n",
    "print(f\"Extracted text from {len(documents)} PDFs.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0009927",
   "metadata": {},
   "source": [
    "## Tokenize documents into sentences of words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "49356350",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenized 1 documents.\n"
     ]
    }
   ],
   "source": [
    "# Tokenize each document into words\n",
    "tokenized_docs = [word_tokenize(doc.lower()) for doc in documents]\n",
    "tokenized_docs = [[word for word in tokens if word.isalpha()] for tokens in tokenized_docs]\n",
    "\n",
    "print(f\"Tokenized {len(tokenized_docs)} documents.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "f5fdab86-bd30-4a7f-8746-3471afbc0544",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenized 1 documents.\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import string\n",
    "\n",
    "stop_words = set(stopwords.words(\"english\"))  # cambia si es español\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "tokenized_docs = []\n",
    "for doc in documents:\n",
    "    tokens = word_tokenize(doc.lower())\n",
    "    clean_tokens = [\n",
    "        lemmatizer.lemmatize(tok)\n",
    "        for tok in tokens\n",
    "            if tok.isalpha() and tok not in stop_words\n",
    "    ]\n",
    "    tokenized_docs.append(clean_tokens)\n",
    "\n",
    "print(f\"Tokenized {len(tokenized_docs)} documents.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccec9326",
   "metadata": {},
   "source": [
    "## Train a Embedding model\n",
    "\n",
    "| Parameter     | Description                                                                                                                |\n",
    "| ------------- | -------------------------------------------------------------------------------------------------------------------------- |\n",
    "| `sentences`   | Input corpus: a list of tokenized documents or sentences.<br>Example: `[['data', 'analysis']]`.                            |\n",
    "| `vector_size` | Dimensionality of the word vectors.<br>Each word is represented as a 100-dimensional vector.                               |\n",
    "| `window`      | Maximum distance between the current and context word.<br>`window=5` looks 5 words ahead and behind.                       |\n",
    "| `min_count`   | Ignores all words with frequency lower than this value.<br>`min_count=1` means **all** words are included.                 |\n",
    "| `workers`     | Number of threads (CPU cores) used during training.                                                                        |\n",
    "| `sg`          | Training algorithm:<br>`sg=1` → **Skip-Gram** (good for rare words)<br>`sg=0` → **CBOW** (faster, good for frequent words) |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e09596d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model trained.\n"
     ]
    }
   ],
   "source": [
    "# Train model\n",
    "w2v_model = Word2Vec(\n",
    "    sentences=tokenized_docs,\n",
    "    vector_size=300,    # Dimensionalidad de los vectores\n",
    "    window=3,           # Ventana de contexto\n",
    "    min_count=0,        # Incluir todas las palabras, incluso las menos frecuentes\n",
    "    workers=4,          # Número de hilos para entrenamiento\n",
    "    sg=1,               # Skip-gram\n",
    "    epochs=30,          # Número de épocas de entrenamiento \n",
    "    negative=15,        # Número de muestras negativas\n",
    "    alpha=0.025,        # Learning rate inicial\n",
    "    min_alpha=0.0001    # Learning rate final\n",
    ")\n",
    "\n",
    "print(\"Model trained.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "feb85ae1-429f-40a0-92d1-814239caa103",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('occupancy', 0.9722837209701538), ('revpar', 0.9558929800987244), ('occupied', 0.9245179891586304), ('occur', 0.9233198761940002), ('wholesale', 0.9230706691741943), ('available', 0.9227901101112366), ('daily', 0.9053273797035217), ('visible', 0.9022544026374817), ('low', 0.8949389457702637), ('always', 0.8903062343597412)]\n"
     ]
    }
   ],
   "source": [
    "print(w2v_model.wv.most_similar(\"intelligence\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "e3295c61-a932-4fb8-8ce4-c662497ca82b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7228476\n"
     ]
    }
   ],
   "source": [
    "print(w2v_model.wv.similarity(\"data\", \"information\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e71b9aa9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('occupancy', 0.8171892762184143), ('available', 0.7895846962928772), ('current', 0.7569169402122498), ('low', 0.7568923830986023), ('actual', 0.7307227253913879), ('occur', 0.729524552822113), ('virtue', 0.7291816473007202), ('covered', 0.7208757996559143), ('budget', 0.7164059281349182), ('period', 0.7149717211723328)]\n"
     ]
    }
   ],
   "source": [
    "print(w2v_model.wv.most_similar(positive=['data', 'business'],negative=['information']))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3318394f",
   "metadata": {},
   "source": [
    "## Save the model for use in Flair"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "0a1a0638",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embeddings saved to 'custom_embeddings.bin'\n"
     ]
    }
   ],
   "source": [
    "output_path = Path(\"../models/word2vec\")\n",
    "output_path.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Save in Word2Vec text format\n",
    "w2v_model.wv.save_word2vec_format(output_path / \"custom_embeddings.bin\", binary=True)\n",
    "print(\"Embeddings saved to 'custom_embeddings.bin'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abce66fd",
   "metadata": {},
   "source": [
    "## Load custom embeddings in Flair"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "05ec36ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Custom embeddings loaded in Flair\n"
     ]
    }
   ],
   "source": [
    "from flair.embeddings import WordEmbeddings\n",
    "embedding = WordEmbeddings(str(output_path / \"custom_embeddings.bin\"))\n",
    "print(\"Custom embeddings loaded in Flair\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2cd9c9ee-b660-433d-b8dd-26e920fa8557",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 1715 unique keywords.\n"
     ]
    }
   ],
   "source": [
    "# Load reports.csv and extract keywords\n",
    "import pandas as pd\n",
    "reports_path = Path(\"../api/reports.csv\")\n",
    "df = pd.read_csv(reports_path).fillna(\"\")\n",
    "\n",
    "keywords = set()\n",
    "for kw_list in df[\"keywords\"]:\n",
    "    kws = [k.strip().lower() for k in kw_list.split(\",\") if k.strip()]\n",
    "    keywords.update(kws)\n",
    "keywords = sorted(keywords)\n",
    "print(f\"Loaded {len(keywords)} unique keywords.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "869534e6-7639-4f4b-9baa-d81c6e90ace2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedded 1715 keywords.\n"
     ]
    }
   ],
   "source": [
    "# Embed each keyword and build a dictionary\n",
    "from flair.data import Sentence\n",
    "import numpy as np\n",
    "keyword_vectors = {}\n",
    "for kw in keywords:\n",
    "    sentence = Sentence(kw, use_tokenizer=True)\n",
    "    embedding.embed(sentence)\n",
    "    if sentence:\n",
    "        # calculate a mean value between word embeddings (for keyphrases)\n",
    "        vector = np.mean([token.embedding.cpu().numpy() for token in sentence], axis=0)\n",
    "        keyword_vectors[kw] = vector        \n",
    "\n",
    "print(f\"Embedded {len(keyword_vectors)} keywords.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6363e69-1ec2-49c7-8a78-9b59ac879c08",
   "metadata": {},
   "source": [
    "## Search for keywords similar to a given query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "01968ffc-038b-43da-a94b-92c4279d9a8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top keywords similar to 'supermarket':\n",
      "\n",
      "process error: 0.9957\n",
      "sale level: 0.9957\n",
      "competitive quality: 0.9956\n",
      "competitive set: 0.9956\n",
      "agency or company level: 0.9955\n",
      "company or hotel level: 0.9955\n",
      "top_company information: 0.9955\n",
      "data governance and business intelligence: 0.9955\n",
      "total revenue and month level: 0.9955\n",
      "contract information: 0.9955\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "query = \"supermarket\"\n",
    "query_sentence = Sentence(query, use_tokenizer=True)\n",
    "embedding.embed(query_sentence)\n",
    "\n",
    "if query_sentence:\n",
    "    query_vector = np.mean([token.embedding.cpu().numpy() for token in query_sentence], axis=0).reshape(1, -1)\n",
    "    scores = {}\n",
    "    for kw, vec in keyword_vectors.items():\n",
    "        sim = cosine_similarity(query_vector, vec.reshape(1, -1))[0][0]\n",
    "        scores[kw] = sim\n",
    "\n",
    "    top_k = sorted(scores.items(), key=lambda x: x[1], reverse=True)[:10]\n",
    "    print(f\"Top keywords similar to '{query}':\\n\")\n",
    "    for kw, score in top_k:\n",
    "        print(f\"{kw}: {score:.4f}\")\n",
    "else:\n",
    "    print(f\"'{query}' could not be embedded.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05f6baa6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

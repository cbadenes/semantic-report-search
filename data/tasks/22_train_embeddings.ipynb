{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "07ac5f62",
   "metadata": {},
   "source": [
    "# Train Custom Word Embeddings from PDF Corpus\n",
    "\n",
    "In this notebook, we train a custom word embedding model using text extracted from a folder of PDF documents. The resulting embeddings can later be loaded and used with Flair for downstream tasks like similarity search, clustering, etc.\n",
    "\n",
    "We will use `PyMuPDF` to extract text from PDFs and `Gensim` to train a Word2Vec model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "15ce2361",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/cbadenes/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "from pathlib import Path\n",
    "import fitz  # PyMuPDF\n",
    "from gensim.models import Word2Vec\n",
    "from nltk.tokenize import word_tokenize\n",
    "import nltk\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b168cbaa",
   "metadata": {},
   "source": [
    "## Extract and clean text from PDF files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e5137294",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reading ../pdf/2006-business-intelligence-20-moving-to-real-time-bi-report-nicholls.pdf\n",
      "Extracted text from 1 PDFs.\n"
     ]
    }
   ],
   "source": [
    "# Path to folder containing PDF files\n",
    "pdf_folder = Path(\"../pdf\")\n",
    "documents = []\n",
    "\n",
    "for pdf_file in pdf_folder.glob(\"*.pdf\"):\n",
    "    print(\"reading\",pdf_file)\n",
    "    text = \"\"\n",
    "    with fitz.open(pdf_file) as doc:\n",
    "        for page in doc:\n",
    "            text += page.get_text()\n",
    "    # Basic cleanup\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "    documents.append(text)\n",
    "\n",
    "print(f\"Extracted text from {len(documents)} PDFs.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0009927",
   "metadata": {},
   "source": [
    "## Tokenize documents into sentences of words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "49356350",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenized 1 documents.\n"
     ]
    }
   ],
   "source": [
    "# Tokenize each document into words\n",
    "tokenized_docs = [word_tokenize(doc.lower()) for doc in documents]\n",
    "tokenized_docs = [[word for word in tokens if word.isalpha()] for tokens in tokenized_docs]\n",
    "\n",
    "print(f\"Tokenized {len(tokenized_docs)} documents.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "f5fdab86-bd30-4a7f-8746-3471afbc0544",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenized 1 documents.\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import string\n",
    "\n",
    "stop_words = set(stopwords.words(\"english\"))  # cambia si es español\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "tokenized_docs = []\n",
    "for doc in documents:\n",
    "    tokens = word_tokenize(doc.lower())\n",
    "    clean_tokens = [\n",
    "        lemmatizer.lemmatize(tok)\n",
    "        for tok in tokens\n",
    "        if tok.isalpha() and tok not in stop_words\n",
    "    ]\n",
    "    tokenized_docs.append(clean_tokens)\n",
    "\n",
    "print(f\"Tokenized {len(tokenized_docs)} documents.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccec9326",
   "metadata": {},
   "source": [
    "## Train a Embedding model\n",
    "\n",
    "| Parameter     | Description                                                                                                                |\n",
    "| ------------- | -------------------------------------------------------------------------------------------------------------------------- |\n",
    "| `sentences`   | Input corpus: a list of tokenized documents or sentences.<br>Example: `[['data', 'analysis']]`.                            |\n",
    "| `vector_size` | Dimensionality of the word vectors.<br>Each word is represented as a 100-dimensional vector.                               |\n",
    "| `window`      | Maximum distance between the current and context word.<br>`window=5` looks 5 words ahead and behind.                       |\n",
    "| `min_count`   | Ignores all words with frequency lower than this value.<br>`min_count=1` means **all** words are included.                 |\n",
    "| `workers`     | Number of threads (CPU cores) used during training.                                                                        |\n",
    "| `sg`          | Training algorithm:<br>`sg=1` → **Skip-Gram** (good for rare words)<br>`sg=0` → **CBOW** (faster, good for frequent words) |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "ae50c349",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model trained.\n"
     ]
    }
   ],
   "source": [
    "# Train model\n",
    "w2v_model = Word2Vec(\n",
    "    sentences=tokenized_docs,\n",
    "    vector_size=100,\n",
    "    window=5,\n",
    "    min_count=1,\n",
    "    workers=4,\n",
    "    sg=1\n",
    ")\n",
    "\n",
    "print(\"Model trained.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "feb85ae1-429f-40a0-92d1-814239caa103",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('business', 0.9985243678092957), ('stream', 0.998451828956604), ('company', 0.9983727931976318), ('value', 0.9983698725700378), ('information', 0.9983309507369995), ('world', 0.9982976317405701), ('process', 0.9982869029045105), ('application', 0.9982665777206421), ('u', 0.9982622265815735), ('automatically', 0.9982519745826721)]\n"
     ]
    }
   ],
   "source": [
    "print(w2v_model.wv.most_similar(\"intelligence\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e3295c61-a932-4fb8-8ce4-c662497ca82b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9982693\n"
     ]
    }
   ],
   "source": [
    "print(w2v_model.wv.similarity(\"data\", \"information\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3318394f",
   "metadata": {},
   "source": [
    "## Save the model for use in Flair"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0a1a0638",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embeddings saved to 'custom_embeddings.bin'\n"
     ]
    }
   ],
   "source": [
    "output_path = Path(\"../models/word2vec\")\n",
    "output_path.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Save in Word2Vec text format\n",
    "w2v_model.wv.save_word2vec_format(output_path / \"custom_embeddings.bin\", binary=True)\n",
    "print(\"Embeddings saved to 'custom_embeddings.bin'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abce66fd",
   "metadata": {},
   "source": [
    "## Load custom embeddings in Flair"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "05ec36ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Custom embeddings loaded in Flair\n"
     ]
    }
   ],
   "source": [
    "from flair.embeddings import WordEmbeddings\n",
    "embedding = WordEmbeddings(str(output_path / \"custom_embeddings.bin\"))\n",
    "print(\"Custom embeddings loaded in Flair\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2cd9c9ee-b660-433d-b8dd-26e920fa8557",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 127 unique keywords.\n"
     ]
    }
   ],
   "source": [
    "# Load reports.csv and extract keywords\n",
    "import pandas as pd\n",
    "reports_path = Path(\"../api/reports.csv\")\n",
    "df = pd.read_csv(reports_path).fillna(\"\")\n",
    "\n",
    "keywords = set()\n",
    "for kw_list in df[\"keywords\"]:\n",
    "    kws = [k.strip().lower() for k in kw_list.split(\",\") if k.strip()]\n",
    "    keywords.update(kws)\n",
    "keywords = sorted(keywords)\n",
    "print(f\"Loaded {len(keywords)} unique keywords.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "869534e6-7639-4f4b-9baa-d81c6e90ace2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedded 127 keywords.\n"
     ]
    }
   ],
   "source": [
    "# Embed each keyword and build a dictionary\n",
    "from flair.data import Sentence\n",
    "import numpy as np\n",
    "keyword_vectors = {}\n",
    "for kw in keywords:\n",
    "    sentence = Sentence(kw, use_tokenizer=True)\n",
    "    embedding.embed(sentence)\n",
    "    if sentence:\n",
    "        # calculate a mean value between word embeddings (for keyphrases)\n",
    "        vector = np.mean([token.embedding.cpu().numpy() for token in sentence], axis=0)\n",
    "        keyword_vectors[kw] = vector        \n",
    "\n",
    "print(f\"Embedded {len(keyword_vectors)} keywords.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6363e69-1ec2-49c7-8a78-9b59ac879c08",
   "metadata": {},
   "source": [
    "## Search for keywords similar to a given query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "01968ffc-038b-43da-a94b-92c4279d9a8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top keywords similar to 'intelligence':\n",
      "\n",
      "competetive set information: 0.9986\n",
      "point of sale: 0.9985\n",
      "diamonds information: 0.9983\n",
      "conversion analysis: 0.9981\n",
      "denial analysis: 0.9981\n",
      "total revenue: 0.9980\n",
      "service: 0.9978\n",
      "oco hotel master data: 0.9976\n",
      "performance: 0.9975\n",
      "global kpis: 0.9975\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "query = \"intelligence\"\n",
    "query_sentence = Sentence(query, use_tokenizer=True)\n",
    "embedding.embed(query_sentence)\n",
    "\n",
    "if query_sentence:\n",
    "    query_vector = np.mean([token.embedding.cpu().numpy() for token in query_sentence], axis=0).reshape(1, -1)\n",
    "    scores = {}\n",
    "    for kw, vec in keyword_vectors.items():\n",
    "        sim = cosine_similarity(query_vector, vec.reshape(1, -1))[0][0]\n",
    "        scores[kw] = sim\n",
    "\n",
    "    top_k = sorted(scores.items(), key=lambda x: x[1], reverse=True)[:10]\n",
    "    print(f\"Top keywords similar to '{query}':\\n\")\n",
    "    for kw, score in top_k:\n",
    "        print(f\"{kw}: {score:.4f}\")\n",
    "else:\n",
    "    print(f\"'{query}' could not be embedded.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

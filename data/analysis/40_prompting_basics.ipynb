{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bA05XuYXYBEr"
      },
      "source": [
        "# Introduction to Prompt Engineering with Large Language Models\n",
        "\n",
        "In this notebook, you'll learn how to interact with large language models (LLMs) using prompting.\n",
        "\n",
        "We will explore:\n",
        "\n",
        "- Basic prompts\n",
        "- Improving prompts with context\n",
        "- Few-shot examples\n",
        "- Using prompt templates\n",
        "\n",
        "<a href=\"https://colab.research.google.com/github/cbadenes/semantic-report-search/blob/main/data/analysis/40_prompting_basics.ipynb\" target=\"_parent\">\n",
        "  <img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open in Colab\"/>\n",
        "</a>\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IsxlWad5eaFA",
        "outputId": "eb2239a2-808a-419b-a346-b5cfa385b6c1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " Enter your Hugging Face token: 路路路路路路路路路路\n"
          ]
        }
      ],
      "source": [
        "from huggingface_hub import login\n",
        "import getpass\n",
        "\n",
        "token = getpass.getpass(\" Enter your Hugging Face token: \")\n",
        "login(token)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ABX68XwKjX43"
      },
      "source": [
        "### Parameters for `pipeline(\"text-generation\", ...)`\n",
        "\n",
        "- **model**: The preloaded model (e.g., `AutoModelForCausalLM`).\n",
        "- **tokenizer**: The tokenizer that matches the model (e.g., `AutoTokenizer`).\n",
        "\n",
        "#### Generation Parameters:\n",
        "\n",
        "- **max_length**:\n",
        "  - Total length of input + generated output.\n",
        "  - Use this for absolute control over sequence size.\n",
        "  - Can be overridden by `max_new_tokens`.\n",
        "\n",
        "- **max_new_tokens**:\n",
        "  - Limits the number of *new* tokens the model should generate.\n",
        "  - Use this instead of `max_length` when input varies in length.\n",
        "  - Recommended for most prompting scenarios.\n",
        "\n",
        "- **truncation**:\n",
        "  - If `True`, cuts input to fit within model limits.\n",
        "  - Useful when feeding long text as prompt.\n",
        "\n",
        "- **do_sample**:\n",
        "  - If `True`, the model samples from the probability distribution (randomness).\n",
        "  - If `False`, it picks the most likely next token (greedy decoding).\n",
        "  - Recommended: `True` + `temperature` tuning for creativity.\n",
        "\n",
        "- **temperature**:\n",
        "  - Controls the randomness of sampling.\n",
        "  - Lower = more deterministic (e.g., 0.3), higher = more creative (e.g., 0.8).\n",
        "  - Best used with `do_sample=True`.\n",
        "\n",
        "- **return_full_text**:\n",
        "  - If `True`, the result includes both the prompt and generated text.\n",
        "  - If `False`, it returns only the model's output.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HDc15w0_YK3X",
        "outputId": "e85b835d-dcb4-46e1-f072-eeb64b7af5d1"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Device set to use cuda:0\n"
          ]
        }
      ],
      "source": [
        "!pip install -q transformers accelerate\n",
        "\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline\n",
        "\n",
        "model_id = \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\"\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_id, token=token)\n",
        "model = AutoModelForCausalLM.from_pretrained(model_id, device_map=\"auto\", token=token)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mPaI9WF6ZL-u",
        "outputId": "ed47a943-118e-439b-e7f3-6268d4ad3a56"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Device set to use cuda:0\n"
          ]
        }
      ],
      "source": [
        "generator = pipeline(\n",
        "    \"text-generation\",\n",
        "    model=model,\n",
        "    tokenizer=tokenizer,\n",
        "    max_length=512,          # Total max length of input + output\n",
        "    truncation=True,         # Truncate input if it exceeds model's max length\n",
        "    do_sample=True,          # Enable sampling (randomness); set False for greedy decoding\n",
        "    return_full_text=True,  # If True, returns input + output; if False, only generated part\n",
        "    temperature=0.9,         # Controls randomness; lower = more deterministic\n",
        "    max_new_tokens=10       # Number of tokens to generate (output only)\n",
        ")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6GymAuVhYUQK"
      },
      "source": [
        "Basic Prompt:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gODa1bYTYSB0",
        "outputId": "0837a1cf-5842-408f-cdc9-6796aabbf7b0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "<|system|>\n",
            "You are a helpful assistant.\n",
            "<|user|>\n",
            "Summarize the purpose of a report that analyzes the distribution flows in feeder markets.\n",
            "<|assistant|>\n",
            "A report analyzing the distribution flows in feeder\n"
          ]
        }
      ],
      "source": [
        "prompt = (\n",
        "    \"<|system|>\\n\"\n",
        "    \"You are a helpful assistant.\\n\"\n",
        "    \"<|user|>\\n\"\n",
        "    \"Summarize the purpose of a report that analyzes the distribution flows in feeder markets.\\n\"\n",
        "    \"<|assistant|>\\n\"\n",
        ")\n",
        "response = generator(prompt)[0]['generated_text']\n",
        "print(response)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Exercise: Generating a Quality Description from Structured Metadata\n",
        "\n",
        "In this exercise, you will build a prompt to help a language model generate a high-quality description for a business report based on structured metadata.\n",
        "\n",
        "You are given a metadata table for a report with fields such as report name, KPIs, dimensions, and business domain.\n",
        "\n",
        "Your task is to:\n",
        "1. **Create a well-structured prompt** that could be sent to a language model (e.g. ChatGPT) to generate a clear, informative, and context-aware description of the report.\n",
        "2. **Compare the generated description** with the original one written by a human expert.\n",
        "\n",
        "| Field | Value |\n",
        "|-------|-------|\n",
        "| ID Data Product | RPPBI0004 |\n",
        "| Report Name | eCommerce Report 2024 |\n",
        "| Product Owner | Bradley Garcia |\n",
        "| PBIX_File | ProductionReport.pbix |\n",
        "| Report View | B2B Digital Report |\n",
        "| Category | Functional |\n",
        "| Description (original) | Specific view to analyze B2B Digital performance. Focused on Total Revenue, segmented by Channel, Brand, and Sub BU. |\n",
        "| Dimensions | Channel, Subchannel, Sub BU, Feeder Market, Brand, Source System, Target Market, Country, Customer Segment |\n",
        "| KPIs | Total Revenue |\n",
        "| Other Terms | CCG, CGW, CRO B2B Digital, NHPro Agencies, NHP, Digital Share |\n",
        "| Tags | B2B Digital |\n",
        "| Filters | No |\n",
        "| Priority | Priority 1 |\n",
        "\n",
        "\n",
        "Write a prompt that could be used to generate a one-paragraph description of the report. The model should:\n",
        "\n",
        "- Explain the purpose of the report\n",
        "- Mention the key metrics and dimensions\n",
        "- Be concise, neutral, and professional\n",
        "- Reflect business relevance\n",
        "\n",
        "Once you generate your prompt and get a model output, evaluate it using the table above and reflect on possible improvements.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "prompt = (\n",
        "    \"<|system|>\\n\"\n",
        "    \"You are a helpful assistant.\\n\"\n",
        "    \"<|user|>\\n\"\n",
        "    \"Summarize the purpose of a report that analyzes the distribution flows in feeder markets.\\n\"\n",
        "    \"<|assistant|>\\n\"\n",
        ")\n",
        "response = generator(prompt)[0]['generated_text']\n",
        "print(response)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Evaluation Criteria\n",
        "\n",
        "You will compare your model-generated description to the original based on the following:\n",
        "\n",
        "| Criterion | Description |\n",
        "|----------|-------------|\n",
        "| **Factual Accuracy** | Does it include the correct metrics and dimensions? |\n",
        "| **Completeness** | Are key elements from the metadata represented? |\n",
        "| **Clarity** | Is the generated description easy to understand? |\n",
        "| **Brevity** | Is the output concise without omitting key info? |\n",
        "| **Tone** | Is the tone appropriate for a business audience? |\n",
        "\n",
        "---\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sentence_transformers import SentenceTransformer, util\n",
        "from sklearn.metrics import precision_score, recall_score, f1_score\n",
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "import numpy as np\n",
        "\n",
        "# Descargar recursos necesarios\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "\n",
        "# Modelo sem谩ntico para embeddings\n",
        "model = SentenceTransformer('all-MiniLM-L6-v2')\n",
        "\n",
        "# Ejemplos de descripci贸n (editable)\n",
        "reference = \"Specific view to analyze B2B Digital performance. Focused on Total Revenue, segmented by Channel, Brand, and Sub BU.\"\n",
        "generated = \"This report provides an overview of B2B Digital performance, emphasizing total revenue and breaking it down by brand, channel, and business unit.\"\n",
        "\n",
        "# 1. Similitud sem谩ntica\n",
        "embedding_ref = model.encode(reference, convert_to_tensor=True)\n",
        "embedding_gen = model.encode(generated, convert_to_tensor=True)\n",
        "semantic_similarity = float(util.pytorch_cos_sim(embedding_ref, embedding_gen))\n",
        "\n",
        "# 2. M茅tricas b谩sicas: precisi贸n, recall, F1 sobre palabras clave\n",
        "def get_keywords(text):\n",
        "    tokens = word_tokenize(text.lower())\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "    return set([w for w in tokens if w.isalnum() and w not in stop_words])\n",
        "\n",
        "ref_keywords = get_keywords(reference)\n",
        "gen_keywords = get_keywords(generated)\n",
        "\n",
        "true_positives = len(ref_keywords & gen_keywords)\n",
        "precision = true_positives / len(gen_keywords) if gen_keywords else 0\n",
        "recall = true_positives / len(ref_keywords) if ref_keywords else 0\n",
        "f1 = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0\n",
        "\n",
        "# 3. Mostrar resultados\n",
        "print(\" Evaluation Summary:\")\n",
        "print(f\"Semantic Similarity (cosine): {semantic_similarity:.3f}\")\n",
        "print(f\"Precision: {precision:.2f}\")\n",
        "print(f\"Recall: {recall:.2f}\")\n",
        "print(f\"F1 Score: {f1:.2f}\")\n",
        "print(f\"Reference Keywords: {sorted(ref_keywords)}\")\n",
        "print(f\"Generated Keywords: {sorted(gen_keywords)}\")\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}

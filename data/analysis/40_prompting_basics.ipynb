{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bA05XuYXYBEr"
   },
   "source": [
    "# Introduction to Prompt Engineering with Large Language Models\n",
    "\n",
    "In this notebook, you'll learn how to interact with large language models (LLMs) using prompting.\n",
    "\n",
    "We will explore:\n",
    "\n",
    "- Basic prompts\n",
    "- Improving prompts with context\n",
    "- Few-shot examples\n",
    "- Using prompt templates\n",
    "\n",
    "<a href=\"https://colab.research.google.com/github/cbadenes/semantic-report-search/blob/main/data/analysis/40_prompting_basics.ipynb\" target=\"_parent\">\n",
    "  <img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open in Colab\"/>\n",
    "</a>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "IsxlWad5eaFA",
    "outputId": "74497cf0-7bf1-4d3d-d893-60afc4789e3f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔑 Enter your Hugging Face token: ··········\n"
     ]
    }
   ],
   "source": [
    "from huggingface_hub import login\n",
    "import getpass\n",
    "\n",
    "token = getpass.getpass(\"🔑 Enter your Hugging Face token: \")\n",
    "login(token)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ABX68XwKjX43"
   },
   "source": [
    "### Parameters for `pipeline(\"text-generation\", ...)`\n",
    "\n",
    "- **model**: The preloaded model (e.g., `AutoModelForCausalLM`).\n",
    "- **tokenizer**: The tokenizer that matches the model (e.g., `AutoTokenizer`).\n",
    "\n",
    "#### Generation Parameters:\n",
    "\n",
    "- **max_length**:\n",
    "  - Total length of input + generated output.\n",
    "  - Use this for absolute control over sequence size.\n",
    "  - ⚠️ Can be overridden by `max_new_tokens`.\n",
    "\n",
    "- **max_new_tokens**:\n",
    "  - Limits the number of *new* tokens the model should generate.\n",
    "  - Use this instead of `max_length` when input varies in length.\n",
    "  - Recommended for most prompting scenarios.\n",
    "\n",
    "- **truncation**:\n",
    "  - If `True`, cuts input to fit within model limits.\n",
    "  - Useful when feeding long text as prompt.\n",
    "\n",
    "- **do_sample**:\n",
    "  - If `True`, the model samples from the probability distribution (randomness).\n",
    "  - If `False`, it picks the most likely next token (greedy decoding).\n",
    "  - Recommended: `True` + `temperature` tuning for creativity.\n",
    "\n",
    "- **temperature**:\n",
    "  - Controls the randomness of sampling.\n",
    "  - Lower = more deterministic (e.g., 0.3), higher = more creative (e.g., 0.8).\n",
    "  - Best used with `do_sample=True`.\n",
    "\n",
    "- **return_full_text**:\n",
    "  - If `True`, the result includes both the prompt and generated text.\n",
    "  - If `False`, it returns only the model's output.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "HDc15w0_YK3X",
    "outputId": "fceb47fc-542d-4e23-a0bd-6326c5694f30"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n"
     ]
    }
   ],
   "source": [
    "!pip install -q transformers accelerate\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline\n",
    "\n",
    "model_id = \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id, token=token)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_id, device_map=\"auto\", token=token)\n",
    "\n",
    "#generator = pipeline(\"text-generation\", model=model, tokenizer=tokenizer)\n",
    "generator = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    max_length=512,          # Total max length of input + output\n",
    "    truncation=True,         # Truncate input if it exceeds model's max length\n",
    "    do_sample=True,          # Enable sampling (randomness); set False for greedy decoding\n",
    "    return_full_text=False,  # If True, returns input + output; if False, only generated part\n",
    "    temperature=0.3,         # Controls randomness; lower = more deterministic\n",
    "    max_new_tokens=100       # Number of tokens to generate (output only)\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6GymAuVhYUQK"
   },
   "source": [
    "Basic Prompt:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "gODa1bYTYSB0",
    "outputId": "fb88ddba-f56b-4172-f2e6-609e512da2fb"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Both `max_new_tokens` (=200) and `max_length`(=512) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A report analyzing the distribution flows in feeder markets is intended to provide insights into the distribution channels and the distribution flows within them. The purpose of such a report is to identify the key players, their distribution channels, and the distribution flows within them. The report may also provide insights into the competitive landscape, market trends, and industry dynamics. The report may also offer recommendations for improving distribution efficiency and reducing costs.\n"
     ]
    }
   ],
   "source": [
    "prompt = (\n",
    "    \"<|system|>\\nYou are a helpful assistant.\\n\"\n",
    "    \"<|user|>\\nSummarize the purpose of a report that analyzes the distribution flows in feeder markets.\\n\"\n",
    "    \"<|assistant|>\\n\"\n",
    ")\n",
    "response = generator(prompt, max_new_tokens=200)[0]['generated_text']\n",
    "print(response)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "e_FFnAj0YX46"
   },
   "source": [
    "Improving Prompt with Context:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "h5Cd9Bv1YYns",
    "outputId": "53efbc81-8583-4dfe-ebe3-53df77933b89"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Both `max_new_tokens` (=300) and `max_length`(=512) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This report provides insights into the contribution of each channel to the market flow across different regions. The report aims to analyze the effectiveness of different marketing strategies and identify areas of improvement for the hospitality industry. The report also highlights the impact of different marketing channels on sales and revenue. The report's purpose is to help hoteliers and marketers understand the overall impact of their marketing strategies and make informed decisions for future campaigns.\n"
     ]
    }
   ],
   "source": [
    "prompt = (\n",
    "    \"<|system|>\\nYou are a helpful assistant that classifies business reports based on their description.\\n\"\n",
    "    \"<|user|>\\n\"\n",
    "    \"Here's an example of a report description and its purpose:\\n\"\n",
    "    \"Description: 'This report provides details about average daily rate (ADR), occupancy, and revenue trends in top-performing hotels.'\\n\"\n",
    "    \"Purpose: 'To analyze revenue performance metrics in hospitality properties.\\n\"\n",
    "    \"Now analyze this one:\\n\"\n",
    "    \"Description: 'Shows contribution of each channel to the market flow across different regions.\\n\"\n",
    "    \"Purpose:\"\n",
    "    \"\\n<|assistant|>\\n\"\n",
    ")\n",
    "\n",
    "response = generator(prompt, max_new_tokens=300)[0]['generated_text']\n",
    "print(response)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9d7-99UrYcHo"
   },
   "source": [
    "Few-Shot Prompting:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "DOhJJylVYdRv",
    "outputId": "5e08336e-8f79-4f42-e19c-c4664ca2d1f4"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Both `max_new_tokens` (=50) and `max_length`(=512) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The report category for this description is 'Market Performance.'\n",
      "The report category for the second description is 'Revenue Management.'\n",
      "The report category for the third description is 'Operations.'\n",
      "The report category for the fourth description is 'Mark\n"
     ]
    }
   ],
   "source": [
    "\n",
    "prompt = (\n",
    "    \"<|system|>\\nYou are a helpful assistant that classifies business reports based on their description.\\n\"\n",
    "    \"<|user|>\\n\"\n",
    "    \"Below are examples of report descriptions and their business categories:\\n\"\n",
    "    \"Description: 'Tracks performance metrics for feeder markets and hotel destinations.'\\nCategory: 'Market Performance'\\n\"\n",
    "    \"Description: 'Analyzes average revenue per room and booking channels.'\\nCategory: 'Revenue Management'\\n\"\n",
    "    \"Description: 'Evaluates staffing and service efficiency across hotel departments.'\\nCategory: 'Operations'\\n\"\n",
    "    \"Now define the category of this one:\\n\"\n",
    "    \"Description: 'Breakdown of regional demand for group bookings.'\\nCategory:\"\n",
    "    \"\\n<|assistant|>\\n\"\n",
    ")\n",
    "response = generator(prompt, max_new_tokens=50)[0]['generated_text']\n",
    "print(response)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TUsXgbaZYgOC"
   },
   "source": [
    "CRISPE Prompt Template (Guided Practice):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "M2qWApZjYgwK",
    "outputId": "ac9f5e54-52ff-4156-ba95-afc17b8cfd8d"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Both `max_new_tokens` (=300) and `max_length`(=512) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Based on the description, the report is classified as Revenue. The key focus is on guest complaints and staff resolution time.\n"
     ]
    }
   ],
   "source": [
    "prompt = (\n",
    "    \"<|system|>\\nYou are a helpful assistant trained to classify hotel business reports based on their descriptions.\\n\"\n",
    "    \"<|user|>\\n\"\n",
    "    \"Context: You're working with a set of hotel business reports.\\n\"\n",
    "    \"Role: You are a data analyst assistant.\\n\"\n",
    "    \"Instructions: Classify the report into one of these categories: ['Market', 'Revenue', 'Operations', 'HR'].\\n\"\n",
    "    \"Steps:\\n\"\n",
    "    \"- Read the description\\n\"\n",
    "    \"- Identify the key business focus\\n\"\n",
    "    \"- Map to a category\\n\\n\"\n",
    "    \"Parameters:\\n\"\n",
    "    \"- Use only one label\\n\"\n",
    "    \"- Be concise\\n\\n\"\n",
    "    \"Example:\\n\"\n",
    "    \"Description: 'Shows ADR and occupancy over time per destination'\\n\"\n",
    "    \"Category: Revenue\\n\\n\"\n",
    "    \"Now classify:\\n\"\n",
    "    \"Description: 'Summarizes guest complaints and staff resolution time'\\n\"\n",
    "    \"Category:\"\n",
    "    \"\\n<|assistant|>\\n\"\n",
    ")\n",
    "\n",
    "response = generator(prompt, max_new_tokens=300)[0]['generated_text']\n",
    "print(response)\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}

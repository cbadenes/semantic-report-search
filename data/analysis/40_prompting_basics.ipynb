{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bA05XuYXYBEr"
      },
      "source": [
        "# Introduction to Prompt Engineering with Large Language Models\n",
        "\n",
        "In this notebook, you'll learn how to interact with large language models (LLMs) using prompting.\n",
        "\n",
        "We will explore:\n",
        "\n",
        "- Basic prompts\n",
        "- Improving prompts with context\n",
        "- Few-shot examples\n",
        "- Using prompt templates\n",
        "\n",
        "<a href=\"https://colab.research.google.com/github/cbadenes/semantic-report-search/blob/main/data/analysis/40_prompting_basics.ipynb\" target=\"_parent\">\n",
        "  <img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open in Colab\"/>\n",
        "</a>\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IsxlWad5eaFA",
        "outputId": "eb2239a2-808a-419b-a346-b5cfa385b6c1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "🔑 Enter your Hugging Face token: ··········\n"
          ]
        }
      ],
      "source": [
        "from huggingface_hub import login\n",
        "import getpass\n",
        "\n",
        "token = getpass.getpass(\"🔑 Enter your Hugging Face token: \")\n",
        "login(token)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ABX68XwKjX43"
      },
      "source": [
        "### Parameters for `pipeline(\"text-generation\", ...)`\n",
        "\n",
        "- **model**: The preloaded model (e.g., `AutoModelForCausalLM`).\n",
        "- **tokenizer**: The tokenizer that matches the model (e.g., `AutoTokenizer`).\n",
        "\n",
        "#### Generation Parameters:\n",
        "\n",
        "- **max_length**:\n",
        "  - Total length of input + generated output.\n",
        "  - Use this for absolute control over sequence size.\n",
        "  - ⚠️ Can be overridden by `max_new_tokens`.\n",
        "\n",
        "- **max_new_tokens**:\n",
        "  - Limits the number of *new* tokens the model should generate.\n",
        "  - Use this instead of `max_length` when input varies in length.\n",
        "  - Recommended for most prompting scenarios.\n",
        "\n",
        "- **truncation**:\n",
        "  - If `True`, cuts input to fit within model limits.\n",
        "  - Useful when feeding long text as prompt.\n",
        "\n",
        "- **do_sample**:\n",
        "  - If `True`, the model samples from the probability distribution (randomness).\n",
        "  - If `False`, it picks the most likely next token (greedy decoding).\n",
        "  - Recommended: `True` + `temperature` tuning for creativity.\n",
        "\n",
        "- **temperature**:\n",
        "  - Controls the randomness of sampling.\n",
        "  - Lower = more deterministic (e.g., 0.3), higher = more creative (e.g., 0.8).\n",
        "  - Best used with `do_sample=True`.\n",
        "\n",
        "- **return_full_text**:\n",
        "  - If `True`, the result includes both the prompt and generated text.\n",
        "  - If `False`, it returns only the model's output.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HDc15w0_YK3X",
        "outputId": "e85b835d-dcb4-46e1-f072-eeb64b7af5d1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Device set to use cuda:0\n"
          ]
        }
      ],
      "source": [
        "!pip install -q transformers accelerate\n",
        "\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline\n",
        "\n",
        "model_id = \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\"\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_id, token=token)\n",
        "model = AutoModelForCausalLM.from_pretrained(model_id, device_map=\"auto\", token=token)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "generator = pipeline(\n",
        "    \"text-generation\",\n",
        "    model=model,\n",
        "    tokenizer=tokenizer,\n",
        "    max_length=512,          # Total max length of input + output\n",
        "    truncation=True,         # Truncate input if it exceeds model's max length\n",
        "    do_sample=True,          # Enable sampling (randomness); set False for greedy decoding\n",
        "    return_full_text=True,  # If True, returns input + output; if False, only generated part\n",
        "    temperature=0.9,         # Controls randomness; lower = more deterministic\n",
        "    max_new_tokens=10       # Number of tokens to generate (output only)\n",
        ")\n"
      ],
      "metadata": {
        "id": "mPaI9WF6ZL-u",
        "outputId": "ed47a943-118e-439b-e7f3-6268d4ad3a56",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Device set to use cuda:0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6GymAuVhYUQK"
      },
      "source": [
        "Basic Prompt:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gODa1bYTYSB0",
        "outputId": "0837a1cf-5842-408f-cdc9-6796aabbf7b0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<|system|>\n",
            "You are a helpful assistant.\n",
            "<|user|>\n",
            "Summarize the purpose of a report that analyzes the distribution flows in feeder markets.\n",
            "<|assistant|>\n",
            "A report analyzing the distribution flows in feeder\n"
          ]
        }
      ],
      "source": [
        "prompt = (\n",
        "    \"<|system|>\\n\"\n",
        "    \"You are a helpful assistant.\\n\"\n",
        "    \"<|user|>\\n\"\n",
        "    \"Summarize the purpose of a report that analyzes the distribution flows in feeder markets.\\n\"\n",
        "    \"<|assistant|>\\n\"\n",
        ")\n",
        "response = generator(prompt)[0]['generated_text']\n",
        "print(response)\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}